{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moustafa-7/ChatBot-Project/blob/master/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSbXJwTgZGxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0c9862d-b9bf-4f2b-8bc1-eb05d99fddd7"
      },
      "source": [
        "!pip install argparse\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from requests.compat import urljoin\n",
        "import gensim"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: argparse in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTKDtW-1ZKY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BotHandler(object):\n",
        "    \"\"\"\n",
        "        BotHandler is a class which implements all back-end of the bot.\n",
        "        It has three main functions:\n",
        "            'get_updates' — checks for new messages\n",
        "            'send_message' – posts new message to user\n",
        "            'get_answer' — computes the most relevant on a user's question\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, token, dialogue_manager):\n",
        "        \n",
        "        self.token = token\n",
        "        self.api_url = \"https://api.telegram.org/bot{}/\".format(token)\n",
        "        self.dialogue_manager = dialogue_manager\n",
        "\n",
        "    def get_updates(self, offset=None, timeout=30):\n",
        "        params = {\"timeout\": timeout, \"offset\": offset}\n",
        "        raw_resp = requests.get(urljoin(self.api_url, \"getUpdates\"), params)\n",
        "        try:\n",
        "            resp = raw_resp.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print(\"Failed to parse response {}: {}.\".format(raw_resp.content, e))\n",
        "            return []\n",
        "\n",
        "        if \"result\" not in resp:\n",
        "            return []\n",
        "        return resp[\"result\"]\n",
        "\n",
        "    def send_message(self, chat_id, text):\n",
        "        params = {\"chat_id\": chat_id, \"text\": text}\n",
        "        return requests.post(urljoin(self.api_url, \"sendMessage\"), params)\n",
        "\n",
        "    def get_answer(self, question):\n",
        "        if question == '/start':\n",
        "            return \"Hi, I am your project bot. How can I help you today?\"\n",
        "        return self.dialogue_manager.generate_answer(question)\n",
        "\n",
        "\n",
        "def is_unicode(text):\n",
        "    return len(text) == len(text.encode())\n",
        "\n",
        "\n",
        "class SimpleDialogueManager(object):\n",
        "    \"\"\"\n",
        "    This is a simple dialogue manager to test the telegram bot.\n",
        "    The main part of our bot will be written here.\n",
        "    \"\"\"\n",
        "    def generate_answer(self, question): \n",
        "        if \"Hi\" in question:\n",
        "            return \"Hello, You\" \n",
        "        else:\n",
        "            return \"Don't be rude. Say Hi first.\"\n",
        "        \n",
        "\n",
        "def main():\n",
        "    # Put your own Telegram Access token here...\n",
        "    token = '828781554:AAEE4sdEf04fZwldjg_mW_8jB7hM8__nuXc'\n",
        "    simple_manager = SimpleDialogueManager()\n",
        "    bot = BotHandler(token, simple_manager)\n",
        "    ###############################################################\n",
        "\n",
        "    print(\"Ready to talk!\")\n",
        "    offset = 0\n",
        "    while True:\n",
        "        updates = bot.get_updates(offset=offset)\n",
        "        for update in updates:\n",
        "            print(\"An update received.\")\n",
        "            if \"message\" in update:\n",
        "                chat_id = update[\"message\"][\"chat\"][\"id\"]\n",
        "                if \"text\" in update[\"message\"]:\n",
        "                    text = update[\"message\"][\"text\"]\n",
        "                    if is_unicode(text):\n",
        "                        print(\"Update content: {}\".format(update))\n",
        "                        bot.send_message(chat_id, bot.get_answer(update[\"message\"][\"text\"]))\n",
        "                    else:\n",
        "                        bot.send_message(chat_id, \"Hmm, you are sending some weird characters to me...\")\n",
        "            offset = max(offset, update['update_id'] + 1)\n",
        "        time.sleep(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEAnzx_uaA75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install chatterbot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ya7T9VYajDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleDialogueManager(object):\n",
        "    \"\"\"\n",
        "    This is a simple dialogue manager to test the telegram bot.\n",
        "    The main part of our bot will be written here.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        from chatterbot import ChatBot\n",
        "        from chatterbot.trainers import ChatterBotCorpusTrainer\n",
        "        chatbot = ChatBot('MLWhizChatterbot')\n",
        "        trainer = ChatterBotCorpusTrainer(chatbot)\n",
        "        trainer.train('chatterbot.corpus.english')\n",
        "        self.chitchat_bot = chatbot\n",
        "\n",
        "    def generate_answer(self, question): \n",
        "        response = self.chitchat_bot.get_response(question)\n",
        "        return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N4Cc5wedjlN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "1160aafe-5a26-418a-afd0-6a1e4ed63eee"
      },
      "source": [
        "#!wget https://github.com/MLWhiz/chatbot/raw/master/data.zip\n",
        "!unzip data.zip -d data\n",
        "os.rmdir('data')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/data/\n",
            "  inflating: data/data/dialogues.tsv  \n",
            "  inflating: data/data/tagged_posts.tsv  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c6f2d243aa7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip data.zip -d data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m: [Errno 39] Directory not empty: 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsJdv9wbftWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d09c511b-2778-412d-89c7-ae60f143a3c2"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K98QsIwscj-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e697df26-4fc3-48dc-da8b-ccc0ba7543c1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dialogues = pd.read_csv(\"data/dialogues.tsv\",sep=\"\\t\")\n",
        "posts = pd.read_csv(\"data/tagged_posts.tsv\",sep=\"\\t\")\n",
        "\n",
        "dialogues.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Like my fear of wearing pastels?</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I figured you'd get to the good stuff eventually.</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text       tag\n",
              "0     Okay -- you're gonna need to learn how to lie.  dialogue\n",
              "1  I'm kidding.  You know how sometimes you just ...  dialogue\n",
              "2                   Like my fear of wearing pastels?  dialogue\n",
              "3  I figured you'd get to the good stuff eventually.  dialogue\n",
              "4  Thank God!  If I had to hear one more story ab...  dialogue"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA1SDY_lco3O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5e2fe4e0-c1c3-4561-a371-7682a0c67252"
      },
      "source": [
        "dialogues.head()\n",
        "posts.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>title</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>Calculate age in C#</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>Filling a DataSet or DataTable from a LINQ que...</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39</td>\n",
              "      <td>Reliable timer in a console application</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42</td>\n",
              "      <td>Best way to allow plugins for a PHP application</td>\n",
              "      <td>php</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59</td>\n",
              "      <td>How do I get a distinct, ordered list of names...</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   post_id                                              title  tag\n",
              "0        9                                Calculate age in C#   c#\n",
              "1       16  Filling a DataSet or DataTable from a LINQ que...   c#\n",
              "2       39            Reliable timer in a console application   c#\n",
              "3       42    Best way to allow plugins for a PHP application  php\n",
              "4       59  How do I get a distinct, ordered list of names...   c#"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF_dGVMDk46-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "066b2052-b017-4216-d61d-b938a9cd61f5"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import train_test_splitnltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "texts  =  list(dialogues[:200000].text.values) + list(posts[:200000].title.values)\n",
        "labels =  ['dialogue']*200000 + ['stackoverflow']*200000\n",
        "data = pd.DataFrame({'text':texts,'target':labels})\n",
        "\n",
        "def text_prepare(text):\n",
        "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
        "    \n",
        "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    bad_symbols_re = re.compile('[^0-9a-z #+_]')\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = replace_by_space_re.sub(' ', text)\n",
        "    text = bad_symbols_re.sub('', text)\n",
        "    text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Doing some data cleaning\n",
        "data['text'] = data['text'].apply(lambda x : text_prepare(x))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'],data['target'],test_size = .1 , random_state=0)\n",
        "\n",
        "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size = 360000, test size = 40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uU8cpOKmDYW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "86623a95-76d2-46db-9408-89242624c791"
      },
      "source": [
        "# We will keep our models and vectorizers in this folder\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def tfidf_features(X_train, X_test, vectorizer_path):\n",
        "    \"\"\"Performs TF-IDF transformation and dumps the model.\"\"\"\n",
        "    tfv = TfidfVectorizer(dtype=np.float32, min_df=3,  max_features=None, \n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
        "            stop_words = 'english')\n",
        "    \n",
        "    X_train = tfv.fit_transform(X_train)\n",
        "    X_test = tfv.transform(X_test)\n",
        "    \n",
        "    pickle.dump(tfv,vectorizer_path)\n",
        "    return X_train, X_test\n",
        "\n",
        "X_train_tfidf, X_test_tfidf = tfidf_features(X_train, X_test, open(\"resources/tfidf.pkl\",'wb'))\n",
        "\n",
        "intent_recognizer = LogisticRegression(C=10,random_state=0)\n",
        "intent_recognizer.fit(X_train_tfidf,y_train)\n",
        "pickle.dump(intent_recognizer, open(\"resources/intent_clf.pkl\" , 'wb'))\n",
        "\n",
        "# Check test accuracy.\n",
        "y_test_pred = intent_recognizer.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print('Test accuracy = {}'.format(test_accuracy))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy = 0.98985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry2rwYQxrxrq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34776df7-55d6-4b37-c9f4-b8a52472e556"
      },
      "source": [
        "# creating the data for Programming Language classifier \n",
        "X = posts['title'].values\n",
        "y = posts['tag'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size = 1737260, test size = 434315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkjkg9Gkvfj7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "31680455-56ca-48da-fa6d-1434a59b6cb0"
      },
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "vectorizer = pickle.load(open(\"resources/tfidf.pkl\", 'rb'))\n",
        "X_train_tfidf, X_test_tfidf = vectorizer.transform(X_train), vectorizer.transform(X_test)\n",
        "tag_classifier = OneVsRestClassifier(LogisticRegression(C=5,random_state=0))\n",
        "tag_classifier.fit(X_train_tfidf,y_train)\n",
        "pickle.dump(tag_classifier, open(\"resources/tag_clf.pkl\", 'wb'))\n",
        "\n",
        "# Check test accuracy.\n",
        "y_test_pred = tag_classifier.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print('Test accuracy = {}'.format(test_accuracy))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-28af41c7ce5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtag_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtag_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resources/tag_clf.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;34m\"not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 self.label_binarizer_.classes_[i]])\n\u001b[0;32m--> 215\u001b[0;31m             for i, column in enumerate(columns))\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(estimator, X, y, classes)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1306\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    924\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFb4tmFsvhqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Google's pre-trained Word2Vec model.\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) \n",
        "#!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "pretrained_embeddings_path = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "w = gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path,binary=True)\n",
        "\n",
        "# from gensim import models\n",
        "\n",
        "# w = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XJC_1j82XTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY-IvMZtvpPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfe85960-e60a-49fb-f43a-1dd68354326b"
      },
      "source": [
        "def question_to_vec(question, embeddings, dim=300):\n",
        "    \"\"\"\n",
        "        question: a string\n",
        "        embeddings: dict where the key is a word and a value is its' embedding\n",
        "        dim: size of the representation\n",
        "\n",
        "        result: vector representation for the question\n",
        "    \"\"\"\n",
        "    word_tokens = question.split(\" \")\n",
        "    question_len = len(word_tokens)\n",
        "    question_mat = np.zeros((question_len,dim), dtype = np.float32)\n",
        "    \n",
        "    for idx, word in enumerate(word_tokens):\n",
        "        if word in embeddings:\n",
        "            question_mat[idx,:] = embeddings[word]\n",
        "            \n",
        "    # remove zero-rows which stand for OOV words       \n",
        "    question_mat = question_mat[~np.all(question_mat == 0, axis = 1)]\n",
        "    \n",
        "    # Compute the mean of each word along the sentence\n",
        "    if question_mat.shape[0] > 0:\n",
        "        vec = np.array(np.mean(question_mat, axis = 0), dtype = np.float32).reshape((1,dim))\n",
        "    else:\n",
        "        vec = np.zeros((1,dim), dtype = np.float32)\n",
        "        \n",
        "    return vec\n",
        "\n",
        "counts_by_tag = posts.groupby(by=['tag'])[\"tag\"].count().reset_index(name = 'count').sort_values(['count'], ascending = False)\n",
        "counts_by_tag = list(zip(counts_by_tag['tag'],counts_by_tag['count']))\n",
        "print(counts_by_tag)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('c#', 394451), ('java', 383456), ('javascript', 375867), ('php', 321752), ('c_cpp', 281300), ('python', 208607), ('ruby', 99930), ('r', 36359), ('vb', 35044), ('swift', 34809)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjZ4INE1vpSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.mkdir('resources/embeddings_folder')\n",
        "for tag, count in counts_by_tag:\n",
        "    tag_posts = posts[posts['tag'] == tag]\n",
        "    tag_post_ids = tag_posts['post_id'].values\n",
        "    tag_vectors = np.zeros((count, 300), dtype=np.float32)\n",
        "    for i, title in enumerate(tag_posts['title']):\n",
        "        tag_vectors[i, :] = question_to_vec(title, model, 300)\n",
        "    # Dump post ids and vectors to a file.\n",
        "    filename = 'resources/embeddings_folder/'+ tag + '.pkl'\n",
        "    pickle.dump((tag_post_ids, tag_vectors), open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZDEOYuXvx4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "98195859-dc5b-49e1-9226-fdf508fd318f"
      },
      "source": [
        "def get_similar_question(question,tag):\n",
        "    # get the path where all question embeddings are kept and load the post_ids and post_embeddings\n",
        "    embeddings_path = 'resources/embeddings_folder/' + tag + \".pkl\"\n",
        "    post_ids, post_embeddings = pickle.load(open(embeddings_path, 'rb'))\n",
        "    # Get the embeddings for the question\n",
        "    question_vec = question_to_vec(question, model, 300)\n",
        "    # find index of most similar post\n",
        "    best_post_index = pairwise_distances_argmin(question_vec,\n",
        "                                                post_embeddings)\n",
        "    # return best post id\n",
        "    return post_ids[best_post_index]\n",
        "\n",
        "get_similar_question(\"how to use list comprehension in python?\",'python')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-1c84691e9892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpost_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_post_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mget_similar_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"how to use list comprehension in python?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-1c84691e9892>\u001b[0m in \u001b[0;36mget_similar_question\u001b[0;34m(question, tag)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mquestion_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# find index of most similar post\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     best_post_index = pairwise_distances_argmin(question_vec,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                                 post_embeddings)\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# return best post id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pairwise_distances_argmin' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19VhHpxkv3Ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
        "\n",
        "# We will need this function to prepare text at prediction time\n",
        "def text_prepare(text):\n",
        "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
        "    \n",
        "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    bad_symbols_re = re.compile('[^0-9a-z #+_]')\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = replace_by_space_re.sub(' ', text)\n",
        "    text = bad_symbols_re.sub('', text)\n",
        "    text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# need this to convert questions asked by user to vectors\n",
        "def question_to_vec(question, embeddings, dim=300):\n",
        "    \"\"\"\n",
        "        question: a string\n",
        "        embeddings: dict where the key is a word and a value is its' embedding\n",
        "        dim: size of the representation\n",
        "\n",
        "        result: vector representation for the question\n",
        "    \"\"\"\n",
        "    word_tokens = question.split(\" \")\n",
        "    question_len = len(word_tokens)\n",
        "    question_mat = np.zeros((question_len,dim), dtype = np.float32)\n",
        "    \n",
        "    for idx, word in enumerate(word_tokens):\n",
        "        if word in embeddings:\n",
        "            question_mat[idx,:] = embeddings[word]\n",
        "            \n",
        "    # remove zero-rows which stand for OOV words       \n",
        "    question_mat = question_mat[~np.all(question_mat == 0, axis = 1)]\n",
        "    \n",
        "    # Compute the mean of each word along the sentence\n",
        "    if question_mat.shape[0] > 0:\n",
        "        vec = np.array(np.mean(question_mat, axis = 0), dtype = np.float32).reshape((1,dim))\n",
        "    else:\n",
        "        vec = np.zeros((1,dim), dtype = np.float32)\n",
        "        \n",
        "    return vec\n",
        "\n",
        "class SimpleDialogueManager(object):\n",
        "    \"\"\"\n",
        "    This is a simple dialogue manager to test the telegram bot.\n",
        "    The main part of our bot will be written here.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "\n",
        "        # Instantiate all the models and TFIDF Objects.\n",
        "        print(\"Loading resources...\")\n",
        "        # Instantiate a Chatterbot for Chitchat type questions\n",
        "        from chatterbot import ChatBot\n",
        "        from chatterbot.trainers import ChatterBotCorpusTrainer\n",
        "        chatbot = ChatBot('MLWhizChatterbot')\n",
        "        trainer = ChatterBotCorpusTrainer(chatbot)\n",
        "        trainer.train('chatterbot.corpus.english')\n",
        "        self.chitchat_bot = chatbot\n",
        "        print(\"Loading Word2vec model...\")\n",
        "        # Instantiate the Google's pre-trained Word2Vec model.\n",
        "        self.model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) \n",
        "        print(\"Loading Classifier objects...\")\n",
        "        # Load the intent classifier and tag classifier\n",
        "        self.intent_recognizer =  pickle.load(open('resources/intent_clf.pkl', 'rb'))\n",
        "        self.tag_classifier =  pickle.load(open('resources/tag_clf.pkl', 'rb'))\n",
        "        # Load the TFIDF vectorizer object\n",
        "        self.tfidf_vectorizer = pickle.load(open('resources/tfidf.pkl', 'rb'))\n",
        "        print(\"Finished Loading Resources\")\n",
        "\n",
        "    # We created this function just above. We just need to have a function to get most similar question's *post id* in the dataset given we know the programming Language of the question. Here it is:\n",
        "    def get_similar_question(self,question,tag):\n",
        "        # get the path where all question embeddings are kept and load the post_ids and post_embeddings\n",
        "        embeddings_path = 'resources/embeddings_folder/' + tag + \".pkl\"\n",
        "        post_ids, post_embeddings = pickle.load(open(embeddings_path, 'rb'))\n",
        "        # Get the embeddings for the question\n",
        "        question_vec = question_to_vec(question, self.model, 300)\n",
        "        # find index of most similar post\n",
        "        best_post_index = pairwise_distances_argmin(question_vec,\n",
        "                                                    post_embeddings)\n",
        "        # return best post id\n",
        "        return post_ids[best_post_index]\n",
        "\n",
        "    def generate_answer(self, question): \n",
        "        prepared_question = text_prepare(question)\n",
        "        features = self.tfidf_vectorizer.transform([prepared_question])\n",
        "        # find intent\n",
        "        intent = self.intent_recognizer.predict(features)[0]\n",
        "        # Chit-chat part:   \n",
        "        if intent == 'dialogue':\n",
        "            response = self.chitchat_bot.get_response(question)\n",
        "        # Stack Overflow Question\n",
        "        else:\n",
        "            # find programming language\n",
        "            tag = self.tag_classifier.predict(features)[0]\n",
        "            # find most similar question post id\n",
        "            post_id = self.get_similar_question(question,tag)[0]\n",
        "            # respond with \n",
        "            response = 'I think its about %s\\nThis thread might help you: https://stackoverflow.com/questions/%s' % (tag, post_id)\n",
        "        return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8pKTcUBv8WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}