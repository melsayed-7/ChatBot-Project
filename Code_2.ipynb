{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moustafa-7/ChatBot-Project/blob/master/Code_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK0GMqTRHK3O",
        "colab_type": "code",
        "outputId": "2b1ffe76-b9c3-4186-bc21-49a41ce92dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSbXJwTgZGxX",
        "colab_type": "code",
        "outputId": "0272d3c8-0a0c-4b3e-84d2-d546ed75c2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install argparse\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from requests.compat import urljoin\n",
        "import gensim\n",
        "from chatterbot import ChatBot\n",
        "from bs4 import BeautifulSoup \n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "import nltk\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import train_test_splitnltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: argparse in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54lsCALZ7I94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9fbb407f-f54d-4120-b459-d2fed624b7b4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OTgNxLz5sqn",
        "colab_type": "code",
        "outputId": "e21cb62e-4498-49f2-c0ce-a9f5ea8fea8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2247
        }
      },
      "source": [
        "!pip install chatterbot\n",
        "!pip install chatterbot-corpus\n",
        "!pip install requests\n",
        "!pip install chatterbot"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting chatterbot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/0e/dac0d82f34f86bf509cf5ef3e2dfc5aa7d444bd843a2330ceb7d854f84f2/ChatterBot-1.0.5-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.9MB/s \n",
            "\u001b[?25hCollecting pint>=0.8.1 (from chatterbot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/9d/bf177ebbc57d25e9e296addc14a1303d1e34d7964af5df428a8332349c42/Pint-0.9-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 14.9MB/s \n",
            "\u001b[?25hCollecting sqlalchemy<1.3,>=1.2 (from chatterbot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/67/d07cf7ac7e6dd0bc55ba62816753f86d7c558107104ca915e730c9ec2512/SQLAlchemy-1.2.19.tar.gz (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 39.0MB/s \n",
            "\u001b[?25hCollecting spacy<2.2,>=2.1 (from chatterbot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/0fab3fa533229436533fb504bb62f4cf7ea29541a487a9d1a0749876fc23/spacy-2.1.4-cp36-cp36m-manylinux1_x86_64.whl (29.8MB)\n",
            "\u001b[K     |████████████████████████████████| 29.8MB 1.6MB/s \n",
            "\u001b[?25hCollecting pyyaml<5.2,>=5.1 (from chatterbot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 41.8MB/s \n",
            "\u001b[?25hCollecting mathparse<0.2,>=0.1 (from chatterbot)\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/e5/4910fb85950cb960fcf3f5aabe1c8e55f5c9201788a1c1302b570a7e1f84/mathparse-0.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from chatterbot) (2018.9)\n",
            "Requirement already satisfied: nltk<4.0,>=3.2 in /usr/local/lib/python3.6/dist-packages (from chatterbot) (3.2.5)\n",
            "Collecting python-dateutil<2.8,>=2.7 (from chatterbot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/68/d87d9b36af36f44254a8d512cbfc48369103a3b9e474be9bdfe536abfc45/python_dateutil-2.7.5-py2.py3-none-any.whl (225kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo<4.0,>=3.3 in /usr/local/lib/python3.6/dist-packages (from chatterbot) (3.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.0.2)\n",
            "Collecting blis<0.3.0,>=0.2.2 (from spacy<2.2,>=2.1->chatterbot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (1.0.2)\n",
            "Collecting thinc<7.1.0,>=7.0.2 (from spacy<2.2,>=2.1->chatterbot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/f1/3df317939a07b2fc81be1a92ac10bf836a1d87b4016346b25f8b63dee321/thinc-7.0.4-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (1.16.3)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (0.9.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.21.0)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.6.0)\n",
            "Collecting srsly<1.1.0,>=0.0.5 (from spacy<2.2,>=2.1->chatterbot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/97/47753e3393aa4b18de9f942fac26f18879d1ae950243a556888f389d1398/srsly-0.0.5-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 53.2MB/s \n",
            "\u001b[?25hCollecting wasabi<1.1.0,>=0.2.0 (from spacy<2.2,>=2.1->chatterbot)\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk<4.0,>=3.2->chatterbot) (1.12.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy<2.2,>=2.1->chatterbot) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2019.3.9)\n",
            "Building wheels for collected packages: sqlalchemy, pyyaml\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/90/a7/3b40c6cc468abff357b38fd075429920bd0d313659d889cf8a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
            "Successfully built sqlalchemy pyyaml\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pint, sqlalchemy, blis, wasabi, srsly, thinc, spacy, pyyaml, mathparse, python-dateutil, chatterbot\n",
            "  Found existing installation: SQLAlchemy 1.3.3\n",
            "    Uninstalling SQLAlchemy-1.3.3:\n",
            "      Successfully uninstalled SQLAlchemy-1.3.3\n",
            "  Found existing installation: thinc 6.12.1\n",
            "    Uninstalling thinc-6.12.1:\n",
            "      Successfully uninstalled thinc-6.12.1\n",
            "  Found existing installation: spacy 2.0.18\n",
            "    Uninstalling spacy-2.0.18:\n",
            "      Successfully uninstalled spacy-2.0.18\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: python-dateutil 2.5.3\n",
            "    Uninstalling python-dateutil-2.5.3:\n",
            "      Successfully uninstalled python-dateutil-2.5.3\n",
            "Successfully installed blis-0.2.4 chatterbot-1.0.5 mathparse-0.1.2 pint-0.9 python-dateutil-2.7.5 pyyaml-5.1 spacy-2.1.4 sqlalchemy-1.2.19 srsly-0.0.5 thinc-7.0.4 wasabi-0.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting chatterbot-corpus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/19/f8b41daf36fe4b0f43e283a820362ffdb2c1128600ab4ee187e84262fa4d/chatterbot_corpus-1.2.0-py2.py3-none-any.whl (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 4.9MB/s \n",
            "\u001b[?25hCollecting PyYAML<4.0,>=3.12 (from chatterbot-corpus)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/a3/1d13970c3f36777c583f136c136f804d70f500168edc1edea6daa7200769/PyYAML-3.13.tar.gz (270kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 15.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/da/0c/74eb680767247273e2cf2723482cb9c924fe70af57c334513f\n",
            "Successfully built PyYAML\n",
            "\u001b[31mERROR: chatterbot 1.0.5 has requirement pyyaml<5.2,>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: PyYAML, chatterbot-corpus\n",
            "  Found existing installation: PyYAML 5.1\n",
            "    Uninstalling PyYAML-5.1:\n",
            "      Successfully uninstalled PyYAML-5.1\n",
            "Successfully installed PyYAML-3.13 chatterbot-corpus-1.2.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: chatterbot in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1 in /usr/local/lib/python3.6/dist-packages (from chatterbot) (2.1.4)\n",
            "Requirement already satisfied: pymongo<4.0,>=3.3 in /usr/local/lib/python3.6/dist-packages (from chatterbot) (3.8.0)\n",
            "Requirement already satisfied: nltk<4.0,>=3.2 in /usr/local/lib/python3.6/dist-packages (from chatterbot) (3.2.5)\n",
            "Requirement already satisfied: mathparse<0.2,>=0.1 in /usr/local/lib/python3.6/dist-packages (from chatterbot) (0.1.2)\n",
            "Requirement already satisfied: sqlalchemy<1.3,>=1.2 in /usr/local/lib/python3.6/dist-packages (from chatterbot) (1.2.19)\n",
            "Requirement already satisfied: python-dateutil<2.8,>=2.7 in /usr/local/lib/python3.6/dist-packages (from chatterbot) (2.7.5)\n",
            "Collecting pyyaml<5.2,>=5.1 (from chatterbot)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from chatterbot) (2018.9)\n",
            "Requirement already satisfied: pint>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from chatterbot) (0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (0.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (1.0.2)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.6.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (0.2.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (0.9.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (1.16.3)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (0.2.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1->chatterbot) (7.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk<4.0,>=3.2->chatterbot) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy<2.2,>=2.1->chatterbot) (4.28.1)\n",
            "\u001b[31mERROR: chatterbot-corpus 1.2.0 has requirement PyYAML<4.0,>=3.12, but you'll have pyyaml 5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyyaml\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgRq1WCT7Y_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19VhHpxkv3Ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
        "\n",
        "# We will need this function to prepare text at prediction time\n",
        "def text_prepare(text):\n",
        "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
        "    \n",
        "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    bad_symbols_re = re.compile('[^0-9a-z #+_]')\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = replace_by_space_re.sub(' ', text)\n",
        "    text = bad_symbols_re.sub('', text)\n",
        "    text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# need this to convert questions asked by user to vectors\n",
        "def question_to_vec(question, embeddings, dim=300):\n",
        "    \"\"\"\n",
        "        question: a string\n",
        "        embeddings: dict where the key is a word and a value is its' embedding\n",
        "        dim: size of the representation\n",
        "\n",
        "        result: vector representation for the question\n",
        "    \"\"\"\n",
        "    word_tokens = question.split(\" \")\n",
        "    question_len = len(word_tokens)\n",
        "    question_mat = np.zeros((question_len,dim), dtype = np.float32)\n",
        "    \n",
        "    for idx, word in enumerate(word_tokens):\n",
        "        if word in embeddings:\n",
        "            question_mat[idx,:] = embeddings[word]\n",
        "            \n",
        "    # remove zero-rows which stand for OOV words       \n",
        "    question_mat = question_mat[~np.all(question_mat == 0, axis = 1)]\n",
        "    \n",
        "    # Compute the mean of each word along the sentence\n",
        "    if question_mat.shape[0] > 0:\n",
        "        vec = np.array(np.mean(question_mat, axis = 0), dtype = np.float32).reshape((1,dim))\n",
        "    else:\n",
        "        vec = np.zeros((1,dim), dtype = np.float32)\n",
        "        \n",
        "    return vec\n",
        "\n",
        "class SimpleDialogueManager_2(object):\n",
        "    \"\"\"\n",
        "    This is a simple dialogue manager to test the telegram bot.\n",
        "    The main part of our bot will be written here.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "\n",
        "        # Instantiate all the models and TFIDF Objects.\n",
        "        print(\"Loading resources...\")\n",
        "        # Instantiate a Chatterbot for Chitchat type questions\n",
        "        from chatterbot import ChatBot\n",
        "        from chatterbot.trainers import ChatterBotCorpusTrainer\n",
        "        chatbot = ChatBot('MLWhizChatterbot')\n",
        "        trainer = ChatterBotCorpusTrainer(chatbot)\n",
        "        trainer.train('chatterbot.corpus.english')\n",
        "        self.chitchat_bot = chatbot\n",
        "        print(\"Loading Word2vec model...\")\n",
        "        # Instantiate the Google's pre-trained Word2Vec model.\n",
        "        self.model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) \n",
        "        print(\"Loading Classifier objects...\")\n",
        "        # Load the intent classifier and tag classifier\n",
        "        self.intent_recognizer =  pickle.load(open('resources/intent_clf.pkl', 'rb'))\n",
        "        self.tag_classifier =  pickle.load(open('resources/tag_clf.pkl', 'rb'))\n",
        "        # Load the TFIDF vectorizer object\n",
        "        self.tfidf_vectorizer = pickle.load(open('resources/tfidf.pkl', 'rb'))\n",
        "        print(\"Finished Loading Resources\")\n",
        "\n",
        "    # We created this function just above. We just need to have a function to get most similar question's *post id* in the dataset given we know the programming Language of the question. Here it is:\n",
        "    def get_similar_question(self,question,tag):\n",
        "        # get the path where all question embeddings are kept and load the post_ids and post_embeddings\n",
        "        embeddings_path = 'resources/embeddings_folder/' + tag + \".pkl\"\n",
        "        post_ids, post_embeddings = pickle.load(open(embeddings_path, 'rb'))\n",
        "        # Get the embeddings for the question\n",
        "        question_vec = question_to_vec(question, self.model, 300)\n",
        "        # find index of most similar post\n",
        "        best_post_index = pairwise_distances_argmin(question_vec,\n",
        "                                                    post_embeddings)\n",
        "        # return best post id\n",
        "        return post_ids[best_post_index]\n",
        "\n",
        "    def generate_answer(self, question): \n",
        "        prepared_question = text_prepare(question)\n",
        "        features = self.tfidf_vectorizer.transform([prepared_question])\n",
        "        # find intent\n",
        "        intent = self.intent_recognizer.predict(features)[0]\n",
        "        # Chit-chat part:   \n",
        "        if intent == 'dialogue':\n",
        "            response = self.chitchat_bot.get_response(question)\n",
        "        # Stack Overflow Question\n",
        "        else:\n",
        "            # find programming language\n",
        "            tag = self.tag_classifier.predict(features)[0]\n",
        "            # find most similar question post id\n",
        "            post_id = self.get_similar_question(question,tag)[0]\n",
        "            # respond with \n",
        "            response = 'I think its about %s\\nThis thread might help you: https://stackoverflow.com/questions/%s' % (tag, post_id)\n",
        "        return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTKDtW-1ZKY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        },
        "outputId": "031e121c-b342-4670-d939-bd7f5a8c7f43"
      },
      "source": [
        "import requests\n",
        "from requests.compat import urljoin\n",
        "import time\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "class BotHandler(object):\n",
        "    \"\"\"\n",
        "        BotHandler is a class which implements all back-end of the bot.\n",
        "        It has three main functions:\n",
        "            'get_updates' — checks for new messages\n",
        "            'send_message' – posts new message to user\n",
        "            'get_answer' — computes the most relevant on a user's question\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, token, dialogue_manager):\n",
        "        \n",
        "        self.token = token\n",
        "        self.api_url = \"https://api.telegram.org/bot{}/\".format(token)\n",
        "        self.dialogue_manager = dialogue_manager\n",
        "\n",
        "    def get_updates(self, offset=None, timeout=30):\n",
        "        params = {\"timeout\": timeout, \"offset\": offset}\n",
        "        raw_resp = requests.get(urljoin(self.api_url, \"getUpdates\"), params)\n",
        "        try:\n",
        "            resp = raw_resp.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print(\"Failed to parse response {}: {}.\".format(raw_resp.content, e))\n",
        "            return []\n",
        "\n",
        "        if \"result\" not in resp:\n",
        "            return []\n",
        "        return resp[\"result\"]\n",
        "\n",
        "    def send_message(self, chat_id, text):\n",
        "        params = {\"chat_id\": chat_id, \"text\": text}\n",
        "        return requests.post(urljoin(self.api_url, \"sendMessage\"), params)\n",
        "\n",
        "    def get_answer(self, question):\n",
        "        if question == '/start':\n",
        "            return \"Hi, I am your project bot. How can I help you today?\"\n",
        "        return self.dialogue_manager.generate_answer(question)\n",
        "\n",
        "\n",
        "def is_unicode(text):\n",
        "    return len(text) == len(text.encode())\n",
        "\n",
        "\n",
        "# class SimpleDialogueManager(object):\n",
        "#     \"\"\"\n",
        "#     This is a simple dialogue manager to test the telegram bot.\n",
        "#     The main part of our bot will be written here.\n",
        "#     \"\"\"\n",
        "#     def generate_answer(self, question): \n",
        "#         if \"Hi\" in question:\n",
        "#             return \"Hello, You\" \n",
        "#         else:\n",
        "#             return \"Don't be rude. Say Hi first.\"\n",
        "        \n",
        "\n",
        "def main():\n",
        "    # Put your own Telegram Access token here...\n",
        "    token = '828781554:AAFeYtCKCs8Ztc2UJdFakDXi4i71UWzlBsU'\n",
        "    simple_manager = SimpleDialogueManager_2()\n",
        "    bot = BotHandler(token, simple_manager)\n",
        "    ###############################################################\n",
        "\n",
        "    print(\"Ready to talk!\")\n",
        "    offset = 0\n",
        "    while True:\n",
        "        updates = bot.get_updates(offset=offset)\n",
        "        for update in updates:\n",
        "            print(\"An update received.\")\n",
        "            if \"message\" in update:\n",
        "                chat_id = update[\"message\"][\"chat\"][\"id\"]\n",
        "                if \"text\" in update[\"message\"]:\n",
        "                    text = update[\"message\"][\"text\"]\n",
        "                    if is_unicode(text):\n",
        "                        print(\"Update content: {}\".format(update))\n",
        "                        bot.send_message(chat_id, bot.get_answer(update[\"message\"][\"text\"]))\n",
        "                    else:\n",
        "                        bot.send_message(chat_id, \"Hmm, you are sending some weird characters to me...\")\n",
        "            offset = max(offset, update['update_id'] + 1)\n",
        "        time.sleep(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Training ai.yml: [###########         ] 55%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/chatterbot/corpus.py:38: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  return yaml.load(data_file)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training ai.yml: [####################] 100%\n",
            "Training botprofile.yml: [####################] 100%\n",
            "Training computers.yml: [####################] 100%\n",
            "\n",
            "Training emotion.yml: [####################] 100%\n",
            "Training food.yml: [####################] 100%\n",
            "Training gossip.yml: [####################] 100%\n",
            "Training greetings.yml: [####################] 100%\n",
            "\n",
            "Training history.yml: [####################] 100%\n",
            "Training humor.yml: [####################] 100%\n",
            "Training literature.yml: [####################] 100%\n",
            "Training money.yml: [####################] 100%\n",
            "\n",
            "Training politics.yml: [####################] 100%\n",
            "Training psychology.yml: [####################] 100%\n",
            "Training science.yml: [####################] 100%\n",
            "Training sports.yml: [####################] 100%\n",
            "Training trivia.yml: [####################] 100%\n",
            "Loading Word2vec model...\n",
            "Loading Classifier objects...\n",
            "Finished Loading Resources\n",
            "Ready to talk!\n",
            "An update received.\n",
            "Update content: {'update_id': 971090324, 'message': {'message_id': 258, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559021387, 'text': 'hello'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090325, 'message': {'message_id': 260, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022505, 'text': 'hello'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090326, 'message': {'message_id': 262, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022513, 'text': 'how to multiply in C++'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090327, 'message': {'message_id': 264, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022524, 'text': 'what is your name?'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090328, 'message': {'message_id': 266, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022545, 'text': 'how things are going'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090329, 'message': {'message_id': 268, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022562, 'text': 'who is peter parker?'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090330, 'message': {'message_id': 270, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022572, 'text': 'who is peter parker?'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090331, 'message': {'message_id': 272, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022585, 'text': 'who is spiderman?'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090332, 'message': {'message_id': 274, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022614, 'text': 'how to append a list in python'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090333, 'message': {'message_id': 276, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022627, 'text': 'How are you?'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090334, 'message': {'message_id': 278, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022635, 'text': 'tell me a joke'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090335, 'message': {'message_id': 280, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022645, 'text': 'who is spiderman?'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090336, 'message': {'message_id': 282, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022702, 'text': 'I am Moustafa Elsayed. nice to meet you'}}\n",
            "An update received.\n",
            "Update content: {'update_id': 971090337, 'message': {'message_id': 284, 'from': {'id': 668280772, 'is_bot': False, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'language_code': 'en'}, 'chat': {'id': 668280772, 'first_name': 'Moustafa', 'last_name': 'Elsayed', 'username': 'melsayed7', 'type': 'private'}, 'date': 1559022715, 'text': \"What's your name?\"}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ya7T9VYajDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleDialogueManager(object):\n",
        "    \"\"\"\n",
        "    This is a simple dialogue manager to test the telegram bot.\n",
        "    The main part of our bot will be written here.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        from chatterbot import ChatBot\n",
        "        from chatterbot.trainers import ChatterBotCorpusTrainer\n",
        "        chatbot = ChatBot('MLWhizChatterbot')\n",
        "        trainer = ChatterBotCorpusTrainer(chatbot)\n",
        "        trainer.train('chatterbot.corpus.english')\n",
        "        self.chitchat_bot = chatbot\n",
        "\n",
        "    def generate_answer(self, question): \n",
        "        response = self.chitchat_bot.get_response(question)\n",
        "        return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N4Cc5wedjlN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "03698928-f46f-4545-965d-9dfeb3fbe2c6"
      },
      "source": [
        "!wget https://github.com/MLWhiz/chatbot/raw/master/data.zip\n",
        "!unzip data.zip -d data\n",
        "# os.rmdir('data')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-28 05:28:36--  https://github.com/MLWhiz/chatbot/raw/master/data.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MLWhiz/chatbot/master/data.zip [following]\n",
            "--2019-05-28 05:28:36--  https://raw.githubusercontent.com/MLWhiz/chatbot/master/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63453527 (61M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  60.51M   237MB/s    in 0.3s    \n",
            "\n",
            "2019-05-28 05:28:37 (237 MB/s) - ‘data.zip’ saved [63453527/63453527]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/data/\n",
            "  inflating: data/data/dialogues.tsv  \n",
            "  inflating: data/data/tagged_posts.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsJdv9wbftWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K98QsIwscj-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "985b65ef-15e8-4ca6-d4d1-39d048fb3c1d"
      },
      "source": [
        "import pandas as pd\n",
        "dialogues = pd.read_csv(\"data/data/dialogues.tsv\",sep=\"\\t\")\n",
        "posts = pd.read_csv(\"data/data/tagged_posts.tsv\",sep=\"\\t\")\n",
        "\n",
        "dialogues.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Like my fear of wearing pastels?</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I figured you'd get to the good stuff eventually.</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text       tag\n",
              "0     Okay -- you're gonna need to learn how to lie.  dialogue\n",
              "1  I'm kidding.  You know how sometimes you just ...  dialogue\n",
              "2                   Like my fear of wearing pastels?  dialogue\n",
              "3  I figured you'd get to the good stuff eventually.  dialogue\n",
              "4  Thank God!  If I had to hear one more story ab...  dialogue"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA1SDY_lco3O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e7460640-b685-4b5e-c142-53ec9a82ad90"
      },
      "source": [
        "dialogues.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Like my fear of wearing pastels?</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I figured you'd get to the good stuff eventually.</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text       tag\n",
              "0     Okay -- you're gonna need to learn how to lie.  dialogue\n",
              "1  I'm kidding.  You know how sometimes you just ...  dialogue\n",
              "2                   Like my fear of wearing pastels?  dialogue\n",
              "3  I figured you'd get to the good stuff eventually.  dialogue\n",
              "4  Thank God!  If I had to hear one more story ab...  dialogue"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF_dGVMDk46-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46d83785-2309-4a3c-9d85-003757c3793b"
      },
      "source": [
        "\n",
        "import re\n",
        "texts  =  list(dialogues[:200000].text.values) + list(posts[:200000].title.values)\n",
        "labels =  ['dialogue']*200000 + ['stackoverflow']*200000\n",
        "data = pd.DataFrame({'text':texts,'target':labels})\n",
        "\n",
        "def text_prepare(text):\n",
        "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
        "    \n",
        "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    bad_symbols_re = re.compile('[^0-9a-z #+_]')\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = replace_by_space_re.sub(' ', text)\n",
        "    text = bad_symbols_re.sub('', text)\n",
        "    text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Doing some data cleaning\n",
        "data['text'] = data['text'].apply(lambda x : text_prepare(x))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'],data['target'],test_size = .1 , random_state=0)\n",
        "\n",
        "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size = 360000, test size = 40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uU8cpOKmDYW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "cb3cba7f-6477-4fdb-d37f-526759d8991e"
      },
      "source": [
        "# We will keep our models and vectorizers in this folder\n",
        "\n",
        "def tfidf_features(X_train, X_test, vectorizer_path):\n",
        "    \"\"\"Performs TF-IDF transformation and dumps the model.\"\"\"\n",
        "    tfv = TfidfVectorizer(dtype=np.float32, min_df=3,  max_features=None, \n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
        "            stop_words = 'english')\n",
        "    \n",
        "    X_train = tfv.fit_transform(X_train)\n",
        "    X_test = tfv.transform(X_test)\n",
        "    \n",
        "    pickle.dump(tfv,vectorizer_path)\n",
        "    return X_train, X_test\n",
        "os.mkdir(\"resources\")\n",
        "X_train_tfidf, X_test_tfidf = tfidf_features(X_train, X_test, open(\"resources/tfidf.pkl\",'wb'))\n",
        "\n",
        "intent_recognizer = LogisticRegression(C=10,random_state=0)\n",
        "intent_recognizer.fit(X_train_tfidf,y_train)\n",
        "pickle.dump(intent_recognizer, open(\"resources/intent_clf.pkl\" , 'wb'))\n",
        "\n",
        "# Check test accuracy.\n",
        "y_test_pred = intent_recognizer.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print('Test accuracy = {}'.format(test_accuracy))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy = 0.98985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry2rwYQxrxrq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "943200e8-c336-4a60-f25b-407904fd7e9b"
      },
      "source": [
        "# creating the data for Programming Language classifier \n",
        "X = posts['title'].values\n",
        "y = posts['tag'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size = 1737260, test size = 434315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkjkg9Gkvfj7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "533f0f9f-bb20-458b-d95f-ffcfa2c28fbf"
      },
      "source": [
        "vectorizer = pickle.load(open(\"resources/tfidf.pkl\", 'rb'))\n",
        "X_train_tfidf, X_test_tfidf = vectorizer.transform(X_train), vectorizer.transform(X_test)\n",
        "tag_classifier = OneVsRestClassifier(LogisticRegression(C=5,random_state=0))\n",
        "tag_classifier.fit(X_train_tfidf,y_train)\n",
        "pickle.dump(tag_classifier, open(\"resources/tag_clf.pkl\", 'wb'))\n",
        "\n",
        "# Check test accuracy.\n",
        "y_test_pred = tag_classifier.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print('Test accuracy = {}'.format(test_accuracy))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy = 0.8043654950899692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFb4tmFsvhqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "ec2b191d-9ba9-4931-ca7e-e7b46bbbf3db"
      },
      "source": [
        "# Load Google's pre-trained Word2Vec model.\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) \n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "pretrained_embeddings_path = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!pip install gensim\n",
        "import gensim\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path,binary=True)\n",
        "\n",
        "# from gensim import models\n",
        "\n",
        "# w = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-28 05:12:49--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.237.189\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.237.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.153)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.153 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.153)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.153->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.153->boto3->smart-open>=1.2.1->gensim) (2.7.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY-IvMZtvpPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1daed0ae-5dc8-4b41-e017-21010b08d63e"
      },
      "source": [
        "def question_to_vec(question, embeddings, dim=300):\n",
        "    \"\"\"\n",
        "        question: a string\n",
        "        embeddings: dict where the key is a word and a value is its' embedding\n",
        "        dim: size of the representation\n",
        "\n",
        "        result: vector representation for the question\n",
        "    \"\"\"\n",
        "    word_tokens = question.split(\" \")\n",
        "    question_len = len(word_tokens)\n",
        "    question_mat = np.zeros((question_len,dim), dtype = np.float32)\n",
        "    \n",
        "    for idx, word in enumerate(word_tokens):\n",
        "        if word in embeddings:\n",
        "            question_mat[idx,:] = embeddings[word]\n",
        "            \n",
        "    # remove zero-rows which stand for OOV words       \n",
        "    question_mat = question_mat[~np.all(question_mat == 0, axis = 1)]\n",
        "    \n",
        "    # Compute the mean of each word along the sentence\n",
        "    if question_mat.shape[0] > 0:\n",
        "        vec = np.array(np.mean(question_mat, axis = 0), dtype = np.float32).reshape((1,dim))\n",
        "    else:\n",
        "        vec = np.zeros((1,dim), dtype = np.float32)\n",
        "        \n",
        "    return vec\n",
        "\n",
        "#from .models import Post\n",
        "counts_by_tag = posts.groupby(by=['tag'])[\"tag\"].count().reset_index(name = 'count').sort_values(['count'], ascending = False)\n",
        "counts_by_tag = list(zip(counts_by_tag['tag'],counts_by_tag['count']))\n",
        "print(counts_by_tag)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('c#', 394451), ('java', 383456), ('javascript', 375867), ('php', 321752), ('c_cpp', 281300), ('python', 208607), ('ruby', 99930), ('r', 36359), ('vb', 35044), ('swift', 34809)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjZ4INE1vpSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "os.mkdir('resources/embeddings_folder')\n",
        "for tag, count in counts_by_tag:\n",
        "    tag_posts = posts[posts['tag'] == tag]\n",
        "    tag_posts\n",
        "    tag_post_ids = tag_posts['post_id'].values\n",
        "    tag_vectors = np.zeros((count, 300), dtype=np.float32)\n",
        "    for i, title in enumerate(tag_posts['title']):\n",
        "        tag_vectors[i, :] = question_to_vec(title, model, 300)\n",
        "    # Dump post ids and vectors to a file.\n",
        "    filename = 'resources/embeddings_folder/'+ tag + '.pkl'\n",
        "    pickle.dump((tag_post_ids, tag_vectors), open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZDEOYuXvx4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03671644-9db6-411b-ffb4-9bcd965bd22e"
      },
      "source": [
        "\n",
        "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
        "def get_similar_question(question,tag):\n",
        "    # get the path where all question embeddings are kept and load the post_ids and post_embeddings\n",
        "    embeddings_path = 'resources/embeddings_folder/' + tag + \".pkl\"\n",
        "    post_ids, post_embeddings = pickle.load(open(embeddings_path, 'rb'))\n",
        "    # Get the embeddings for the question\n",
        "    question_vec = question_to_vec(question, model, 300)\n",
        "    # find index of most similar post\n",
        "    best_post_index = pairwise_distances_argmin(question_vec,\n",
        "                                                post_embeddings)\n",
        "    # return best post id\n",
        "    return post_ids[best_post_index]\n",
        "\n",
        "get_similar_question(\"how to use list comprehension in python?\",'python')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5947137])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8pKTcUBv8WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}